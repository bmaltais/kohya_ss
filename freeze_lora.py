all = ["lora_te_text_model_encoder_layers_0_self_attn_k_proj",
       "lora_te_text_model_encoder_layers_0_self_attn_v_proj",
       "lora_te_text_model_encoder_layers_0_self_attn_q_proj",
       "lora_te_text_model_encoder_layers_0_self_attn_out_proj",
       "lora_te_text_model_encoder_layers_0_mlp_fc1",
       "lora_te_text_model_encoder_layers_0_mlp_fc2",
       "lora_te_text_model_encoder_layers_1_self_attn_k_proj",
       "lora_te_text_model_encoder_layers_1_self_attn_v_proj",
       "lora_te_text_model_encoder_layers_1_self_attn_q_proj",
       "lora_te_text_model_encoder_layers_1_self_attn_out_proj",
       "lora_te_text_model_encoder_layers_1_mlp_fc1",
       "lora_te_text_model_encoder_layers_1_mlp_fc2",
       "lora_te_text_model_encoder_layers_2_self_attn_k_proj",
       "lora_te_text_model_encoder_layers_2_self_attn_v_proj",
       "lora_te_text_model_encoder_layers_2_self_attn_q_proj",
       "lora_te_text_model_encoder_layers_2_self_attn_out_proj",
       "lora_te_text_model_encoder_layers_2_mlp_fc1",
       "lora_te_text_model_encoder_layers_2_mlp_fc2",
       "lora_te_text_model_encoder_layers_3_self_attn_k_proj",
       "lora_te_text_model_encoder_layers_3_self_attn_v_proj",
       "lora_te_text_model_encoder_layers_3_self_attn_q_proj",
       "lora_te_text_model_encoder_layers_3_self_attn_out_proj",
       "lora_te_text_model_encoder_layers_3_mlp_fc1",
       "lora_te_text_model_encoder_layers_3_mlp_fc2",
       "lora_te_text_model_encoder_layers_4_self_attn_k_proj",
       "lora_te_text_model_encoder_layers_4_self_attn_v_proj",
       "lora_te_text_model_encoder_layers_4_self_attn_q_proj",
       "lora_te_text_model_encoder_layers_4_self_attn_out_proj",
       "lora_te_text_model_encoder_layers_4_mlp_fc1",
       "lora_te_text_model_encoder_layers_4_mlp_fc2",
       "lora_te_text_model_encoder_layers_5_self_attn_k_proj",
       "lora_te_text_model_encoder_layers_5_self_attn_v_proj",
       "lora_te_text_model_encoder_layers_5_self_attn_q_proj",
       "lora_te_text_model_encoder_layers_5_self_attn_out_proj",
       "lora_te_text_model_encoder_layers_5_mlp_fc1",
       "lora_te_text_model_encoder_layers_5_mlp_fc2",
       "lora_te_text_model_encoder_layers_6_self_attn_k_proj",
       "lora_te_text_model_encoder_layers_6_self_attn_v_proj",
       "lora_te_text_model_encoder_layers_6_self_attn_q_proj",
       "lora_te_text_model_encoder_layers_6_self_attn_out_proj",
       "lora_te_text_model_encoder_layers_6_mlp_fc1",
       "lora_te_text_model_encoder_layers_6_mlp_fc2",
       "lora_te_text_model_encoder_layers_7_self_attn_k_proj",
       "lora_te_text_model_encoder_layers_7_self_attn_v_proj",
       "lora_te_text_model_encoder_layers_7_self_attn_q_proj",
       "lora_te_text_model_encoder_layers_7_self_attn_out_proj",
       "lora_te_text_model_encoder_layers_7_mlp_fc1",
       "lora_te_text_model_encoder_layers_7_mlp_fc2",
       "lora_te_text_model_encoder_layers_8_self_attn_k_proj",
       "lora_te_text_model_encoder_layers_8_self_attn_v_proj",
       "lora_te_text_model_encoder_layers_8_self_attn_q_proj",
       "lora_te_text_model_encoder_layers_8_self_attn_out_proj",
       "lora_te_text_model_encoder_layers_8_mlp_fc1",
       "lora_te_text_model_encoder_layers_8_mlp_fc2",
       "lora_te_text_model_encoder_layers_9_self_attn_k_proj",
       "lora_te_text_model_encoder_layers_9_self_attn_v_proj",
       "lora_te_text_model_encoder_layers_9_self_attn_q_proj",
       "lora_te_text_model_encoder_layers_9_self_attn_out_proj",
       "lora_te_text_model_encoder_layers_9_mlp_fc1",
       "lora_te_text_model_encoder_layers_9_mlp_fc2",
       "lora_te_text_model_encoder_layers_10_self_attn_k_proj",
       "lora_te_text_model_encoder_layers_10_self_attn_v_proj",
       "lora_te_text_model_encoder_layers_10_self_attn_q_proj",
       "lora_te_text_model_encoder_layers_10_self_attn_out_proj",
       "lora_te_text_model_encoder_layers_10_mlp_fc1",
       "lora_te_text_model_encoder_layers_10_mlp_fc2",
       "lora_te_text_model_encoder_layers_11_self_attn_k_proj",
       "lora_te_text_model_encoder_layers_11_self_attn_v_proj",
       "lora_te_text_model_encoder_layers_11_self_attn_q_proj",
       "lora_te_text_model_encoder_layers_11_self_attn_out_proj",
       "lora_te_text_model_encoder_layers_11_mlp_fc1",
       "lora_te_text_model_encoder_layers_11_mlp_fc2",
       "lora_unet_down_blocks_0_attentions_0_proj_in",
       "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_q",
       "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_k",
       "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_v",
       "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0",
       "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj",
       "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2",
       "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_q",
       "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_k",
       "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_v",
       "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0",
       "lora_unet_down_blocks_0_attentions_0_proj_out",
       "lora_unet_down_blocks_0_attentions_1_proj_in",
       "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_q",
       "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_k",
       "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_v",
       "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0",
       "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj",
       "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2",
       "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_q",
       "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_k",
       "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_v",
       "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0",
       "lora_unet_down_blocks_0_attentions_1_proj_out",
       "lora_unet_down_blocks_1_attentions_0_proj_in",
       "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q",
       "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k",
       "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v",
       "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0",
       "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj",
       "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2",
       "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q",
       "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k",
       "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v",
       "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0",
       "lora_unet_down_blocks_1_attentions_0_proj_out",
       "lora_unet_down_blocks_1_attentions_1_proj_in",
       "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q",
       "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k",
       "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v",
       "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0",
       "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj",
       "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2",
       "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q",
       "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k",
       "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v",
       "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0",
       "lora_unet_down_blocks_1_attentions_1_proj_out",
       "lora_unet_down_blocks_2_attentions_0_proj_in",
       "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q",
       "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k",
       "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v",
       "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0",
       "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj",
       "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2",
       "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q",
       "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k",
       "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v",
       "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0",
       "lora_unet_down_blocks_2_attentions_0_proj_out",
       "lora_unet_down_blocks_2_attentions_1_proj_in",
       "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q",
       "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k",
       "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v",
       "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0",
       "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj",
       "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2",
       "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q",
       "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k",
       "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v",
       "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0",
       "lora_unet_down_blocks_2_attentions_1_proj_out",
       "lora_unet_up_blocks_1_attentions_0_proj_in",
       "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q",
       "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k",
       "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v",
       "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0",
       "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj",
       "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2",
       "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q",
       "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k",
       "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v",
       "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0",
       "lora_unet_up_blocks_1_attentions_0_proj_out",
       "lora_unet_up_blocks_1_attentions_1_proj_in",
       "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q",
       "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k",
       "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v",
       "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0",
       "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj",
       "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2",
       "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q",
       "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k",
       "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v",
       "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0",
       "lora_unet_up_blocks_1_attentions_1_proj_out",
       "lora_unet_up_blocks_1_attentions_2_proj_in",
       "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_q",
       "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_k",
       "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_v",
       "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0",
       "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj",
       "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2",
       "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_q",
       "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_k",
       "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_v",
       "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0",
       "lora_unet_up_blocks_1_attentions_2_proj_out",
       "lora_unet_up_blocks_2_attentions_0_proj_in",
       "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q",
       "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k",
       "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v",
       "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0",
       "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj",
       "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2",
       "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q",
       "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k",
       "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v",
       "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0",
       "lora_unet_up_blocks_2_attentions_0_proj_out",
       "lora_unet_up_blocks_2_attentions_1_proj_in",
       "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q",
       "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k",
       "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v",
       "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0",
       "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj",
       "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2",
       "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q",
       "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k",
       "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v",
       "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0",
       "lora_unet_up_blocks_2_attentions_1_proj_out",
       "lora_unet_up_blocks_2_attentions_2_proj_in",
       "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_q",
       "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_k",
       "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_v",
       "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0",
       "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj",
       "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2",
       "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_q",
       "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_k",
       "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_v",
       "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0",
       "lora_unet_up_blocks_2_attentions_2_proj_out",
       "lora_unet_up_blocks_3_attentions_0_proj_in",
       "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_q",
       "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_k",
       "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_v",
       "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0",
       "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj",
       "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2",
       "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_q",
       "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_k",
       "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_v",
       "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0",
       "lora_unet_up_blocks_3_attentions_0_proj_out",
       "lora_unet_up_blocks_3_attentions_1_proj_in",
       "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_q",
       "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_k",
       "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_v",
       "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0",
       "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj",
       "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2",
       "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_q",
       "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_k",
       "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_v",
       "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0",
       "lora_unet_up_blocks_3_attentions_1_proj_out",
       "lora_unet_up_blocks_3_attentions_2_proj_in",
       "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_q",
       "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_k",
       "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_v",
       "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0",
       "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj",
       "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2",
       "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_q",
       "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_k",
       "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_v",
       "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0",
       "lora_unet_up_blocks_3_attentions_2_proj_out",
       "lora_unet_mid_block_attentions_0_proj_in",
       "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_q",
       "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_k",
       "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_v",
       "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0",
       "lora_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj",
       "lora_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_2",
       "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_q",
       "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_k",
       "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_v",
       "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0",
       "lora_unet_mid_block_attentions_0_proj_out"
       ]

freeze = ["lora_te_text_model_encoder_layers_0_self_attn_k_proj",
          "lora_te_text_model_encoder_layers_0_self_attn_v_proj",
          "lora_te_text_model_encoder_layers_0_self_attn_q_proj",
          "lora_te_text_model_encoder_layers_0_self_attn_out_proj",
          "lora_te_text_model_encoder_layers_0_mlp_fc1",
          "lora_te_text_model_encoder_layers_0_mlp_fc2",
          "lora_te_text_model_encoder_layers_1_self_attn_k_proj",
          "lora_te_text_model_encoder_layers_1_self_attn_v_proj",
          "lora_te_text_model_encoder_layers_1_self_attn_q_proj",
          "lora_te_text_model_encoder_layers_1_self_attn_out_proj",
          "lora_te_text_model_encoder_layers_1_mlp_fc1",
          "lora_te_text_model_encoder_layers_1_mlp_fc2",
          "lora_te_text_model_encoder_layers_2_self_attn_k_proj",
          "lora_te_text_model_encoder_layers_2_self_attn_v_proj",
          "lora_te_text_model_encoder_layers_2_self_attn_q_proj",
          "lora_te_text_model_encoder_layers_2_self_attn_out_proj",
          "lora_te_text_model_encoder_layers_2_mlp_fc1",
          "lora_te_text_model_encoder_layers_2_mlp_fc2",
          "lora_te_text_model_encoder_layers_3_self_attn_k_proj",
          "lora_te_text_model_encoder_layers_3_self_attn_v_proj",
          "lora_te_text_model_encoder_layers_3_self_attn_q_proj",
          "lora_te_text_model_encoder_layers_3_self_attn_out_proj",
          "lora_te_text_model_encoder_layers_3_mlp_fc1",
          "lora_te_text_model_encoder_layers_3_mlp_fc2",
          "lora_te_text_model_encoder_layers_4_self_attn_k_proj",
          "lora_te_text_model_encoder_layers_4_self_attn_v_proj",
          "lora_te_text_model_encoder_layers_4_self_attn_q_proj",
          "lora_te_text_model_encoder_layers_4_self_attn_out_proj",
          "lora_te_text_model_encoder_layers_4_mlp_fc1",
          "lora_te_text_model_encoder_layers_4_mlp_fc2",
          "lora_te_text_model_encoder_layers_5_self_attn_k_proj",
          "lora_te_text_model_encoder_layers_5_self_attn_v_proj",
          "lora_te_text_model_encoder_layers_5_self_attn_q_proj",
          "lora_te_text_model_encoder_layers_5_self_attn_out_proj",
          "lora_te_text_model_encoder_layers_5_mlp_fc1",
          "lora_te_text_model_encoder_layers_5_mlp_fc2",
          "lora_te_text_model_encoder_layers_6_self_attn_k_proj",
          "lora_te_text_model_encoder_layers_6_self_attn_v_proj",
          "lora_te_text_model_encoder_layers_6_self_attn_q_proj",
          "lora_te_text_model_encoder_layers_6_self_attn_out_proj",
          "lora_te_text_model_encoder_layers_6_mlp_fc1",
          "lora_te_text_model_encoder_layers_6_mlp_fc2",
          "lora_te_text_model_encoder_layers_7_self_attn_k_proj",
          "lora_te_text_model_encoder_layers_7_self_attn_v_proj",
          "lora_te_text_model_encoder_layers_7_self_attn_q_proj",
          "lora_te_text_model_encoder_layers_7_self_attn_out_proj",
          "lora_te_text_model_encoder_layers_7_mlp_fc1",
          "lora_te_text_model_encoder_layers_7_mlp_fc2",
          "lora_te_text_model_encoder_layers_8_self_attn_k_proj",
          "lora_te_text_model_encoder_layers_8_self_attn_v_proj",
          "lora_te_text_model_encoder_layers_8_self_attn_q_proj",
          "lora_te_text_model_encoder_layers_8_self_attn_out_proj",
          "lora_te_text_model_encoder_layers_8_mlp_fc1",
          "lora_te_text_model_encoder_layers_8_mlp_fc2",
          "lora_te_text_model_encoder_layers_9_self_attn_k_proj",
          "lora_te_text_model_encoder_layers_9_self_attn_v_proj",
          "lora_te_text_model_encoder_layers_9_self_attn_q_proj",
          "lora_te_text_model_encoder_layers_9_self_attn_out_proj",
          "lora_te_text_model_encoder_layers_9_mlp_fc1",
          "lora_te_text_model_encoder_layers_9_mlp_fc2",
          "lora_te_text_model_encoder_layers_10_self_attn_k_proj",
          "lora_te_text_model_encoder_layers_10_self_attn_v_proj",
          "lora_te_text_model_encoder_layers_10_self_attn_q_proj",
          "lora_te_text_model_encoder_layers_10_self_attn_out_proj",
          "lora_te_text_model_encoder_layers_10_mlp_fc1",
          "lora_te_text_model_encoder_layers_10_mlp_fc2",
          "lora_te_text_model_encoder_layers_11_self_attn_k_proj",
          "lora_te_text_model_encoder_layers_11_self_attn_v_proj",
          "lora_te_text_model_encoder_layers_11_self_attn_q_proj",
          "lora_te_text_model_encoder_layers_11_self_attn_out_proj",
          "lora_te_text_model_encoder_layers_11_mlp_fc1",
          "lora_te_text_model_encoder_layers_11_mlp_fc2",
          "lora_unet_down_blocks_0_attentions_0_proj_in",
          "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_q",
          "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_k",
          "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_v",
          "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0",
          "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj",
          "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2",
          "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_q",
          "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_k",
          "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_v",
          "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0",
          "lora_unet_down_blocks_0_attentions_0_proj_out",
          "lora_unet_down_blocks_0_attentions_1_proj_in",
          "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_q",
          "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_k",
          "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_v",
          "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0",
          "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj",
          "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2",
          "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_q",
          "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_k",
          "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_v",
          "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0",
          "lora_unet_down_blocks_0_attentions_1_proj_out",
          "lora_unet_down_blocks_1_attentions_0_proj_in",
          "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q",
          "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k",
          "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v",
          "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0",
          "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj",
          "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2",
          "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q",
          "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k",
          "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v",
          "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0",
          "lora_unet_down_blocks_1_attentions_0_proj_out",
          "lora_unet_down_blocks_1_attentions_1_proj_in",
          "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q",
          "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k",
          "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v",
          "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0",
          "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj",
          "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2",
          "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q",
          "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k",
          "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v",
          "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0",
          "lora_unet_down_blocks_1_attentions_1_proj_out",
          "lora_unet_up_blocks_3_attentions_0_proj_in",
          "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_q",
          "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_k",
          "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_v",
          "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0",
          "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj",
          "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2",
          "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_q",
          "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_k",
          "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_v",
          "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0",
          "lora_unet_up_blocks_3_attentions_0_proj_out",
          "lora_unet_up_blocks_3_attentions_1_proj_in",
          "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_q",
          "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_k",
          "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_v",
          "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0",
          "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj",
          "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2",
          "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_q",
          "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_k",
          "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_v",
          "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0",
          "lora_unet_up_blocks_3_attentions_1_proj_out",
          "lora_unet_up_blocks_3_attentions_2_proj_in",
          "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_q",
          "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_k",
          "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_v",
          "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0",
          "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj",
          "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2",
          "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_q",
          "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_k",
          "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_v",
          "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0",
          "lora_unet_up_blocks_3_attentions_2_proj_out",
          "lora_unet_mid_block_attentions_0_proj_in",
          "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_q",
          "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_k",
          "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_v",
          "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0",
          "lora_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj",
          "lora_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_2",
          "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_q",
          "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_k",
          "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_v",
          "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0",
          "lora_unet_mid_block_attentions_0_proj_out"
          ]


def freeze_lora(params):
    temp_params = params
    trainable_params = []

    for params in temp_params:
        temp = {'params': [], 'lr': params['lr']}
        for name, param in params['params']:
            if name in freeze:
                temp['params'].append(param.__next__().requires_grad_(True))
                print(name)
            else:
                temp['params'].append(param.__next__().requires_grad_(False))
        trainable_params.append(temp)

    return trainable_params
