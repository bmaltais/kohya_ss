
 {
  "-Need to add resources here": "-éœ€è¦åœ¨æ­¤æ·»åŠ è³‡æ–™",
  "(Experimental, Optional) Since the latent is close to a normal distribution, it may be a good idea to specify a value around 1/10 the noise offset.": " (é¸å¡«ï¼Œå¯¦é©—æ€§åŠŸèƒ½) ç”±æ–¼æ½›ç©ºé–“æ¥è¿‘å¸¸æ…‹åˆ†å¸ƒï¼Œæˆ–è¨±æŒ‡å®šä¸€å€‹å™ªè²åç§»ç´„ 1/10 çš„æ•¸å€¼æ˜¯å€‹ä¸éŒ¯çš„ä½œæ³•ã€‚",
  "(Optional) Add training comment to be included in metadata": " (é¸å¡«) åœ¨è¨“ç·´çš„å¾Œè¨­è³‡æ–™åŠ å…¥è¨»è§£ã€‚",
  "(Optional) Enforce number of epoch": " (é¸å¡«) å¼·åˆ¶æŒ‡å®šä¸€å€‹é€±æœŸ (Epoch) æ•¸é‡",
  "(Optional) Enforce number of steps": " (é¸å¡«) å¼·åˆ¶æŒ‡å®šä¸€å€‹ç¸½æ­¥æ•¸æ•¸é‡",
  "(Optional) Save only the specified number of models (old models will be deleted)": " (é¸å¡«) åƒ…å„²å­˜æŒ‡å®šæ•¸é‡çš„æ¨¡å‹ (èˆŠæœ‰æ¨¡å‹å°‡è¢«åˆªé™¤) ",
  "(Optional) Save only the specified number of states (old models will be deleted)": " (é¸å¡«) åƒ…å„²å­˜æŒ‡å®šæ•¸é‡çš„è¨“ç·´è³‡æ–™ (èˆŠæœ‰è¨“ç·´è³‡æ–™å°‡è¢«åˆªé™¤) ",
  "(Optional) Stable Diffusion base model": " (é¸å¡«) ç©©å®šæ“´æ•£åŸºç¤æ¨¡å‹",
  "(Optional) Stable Diffusion model": " (é¸å¡«) ç©©å®šæ“´æ•£æ¨¡å‹",
  "(Optional) The model is saved every specified steps": " (é¸å¡«) æ¨¡å‹æœƒåœ¨æŒ‡å®šçš„é–“éš”æ­¥æ•¸å¾Œå„²å­˜",
  "(Optional)": " (é¸å¡«) ",
  "Optional": "é¸å¡«",
  "Optional. Se": "é¸å¡«",
  "(Optional) Directory containing the regularisation images": " (é¸å¡«) å«æœ‰æ­£è¦åŒ–åœ–ç‰‡çš„è³‡æ–™å¤¾",
  "Eg: asd": "ä¾‹å¦‚ï¼šasd",
  "Eg: person": "ä¾‹å¦‚ï¼šperson",
  "Folder containing the concepts folders to balance...": "å«æœ‰è¦å¹³è¡¡çš„æ¦‚å¿µè³‡æ–™å¤¾çš„è³‡æ–™å¤¾è·¯å¾‘...",
  "Balance dataset": "å¹³è¡¡è³‡æ–™é›†",
  "Clamp Quantile": "å¤¾å–åˆ†ä½æ•¸",
  "Minimum difference": "æœ€å°åŒ–å·®ç•°",
  "network dim for linear layer in fixed mode": "å›ºå®šæ¨¡å¼ä¸‹ç·šæ€§å±¤çš„ç¶²è·¯ç¶­åº¦",
  "network dim for conv layer in fixed mode": "å›ºå®šæ¨¡å¼ä¸‹å·ç©å±¤çš„ç¶²è·¯ç¶­åº¦",
  "Sparsity for sparse bias": "ç¨€ç–åå·®çš„ç¨€ç–åº¦",
  "path for the file to save...": "å„²å­˜æª”æ¡ˆçš„è·¯å¾‘...",
  "Verify LoRA": "é©—è­‰ LoRA",
  "Verify": "é©—è­‰",
  "Verification output": "é©—è­‰è¼¸å‡º",
  "Verification error": "é©—è­‰éŒ¯èª¤",
  "New Rank": "æ–°ç¶­åº¦ (Network Rank)",
  "New Conv Rank": "æ–°å·ç©ç¶­åº¦ (Conv Rank)",
  "Directory containing the images to group": "å«æœ‰è¦åˆ†çµ„çš„åœ–ç‰‡çš„è³‡æ–™å¤¾è·¯å¾‘",
  "Directory where the grouped images will be stored": "è¦å„²å­˜åˆ†çµ„åœ–ç‰‡çš„è³‡æ–™å¤¾è·¯å¾‘",
  "Group images": "åˆ†çµ„åœ–ç‰‡",
  "Group Images": "åˆ†çµ„åœ–ç‰‡",
  "Captioning": "æ¨™è¨˜æ–‡å­—",
  "Caption images": "æ¨™è¨˜åœ–ç‰‡",
  "(Optional) model id for GIT in Hugging Face": " (é¸å¡«) Hugging Face ä¸­ GIT çš„æ¨¡å‹ ID",
  "Undesired tags": "ä¸éœ€è¦çš„æ¨™ç±¤",
  "(Optional) Separate `undesired_tags` with comma `(,)` if you want to remove multiple tags, e.g. `1girl,solo,smile`.": " (é¸å¡«) å¦‚æœè¦ç§»é™¤å¤šå€‹æ¨™ç±¤ï¼Œè«‹ä½¿ç”¨é€—è™Ÿ `(,)` åˆ†éš” `undesired_tags`ï¼Œä¾‹å¦‚ï¼š`1girl,solo,smile`ã€‚",
  "Prefix to add to WD14 caption": "è¦åŠ å…¥åˆ° WD14 æ¨™è¨˜æ–‡å­—çš„å‰ç¶´",
  "Postfix to add to WD14 caption": "è¦åŠ å…¥åˆ° WD14 æ¨™è¨˜æ–‡å­—çš„å¾Œç¶´",
  "This option appends the tags to the existing tags, instead of replacing them.": "æ­¤é¸é …å°‡æ¨™ç±¤é™„åŠ åˆ°ç¾æœ‰æ¨™ç±¤ï¼Œè€Œä¸æ˜¯å–ä»£å®ƒå€‘ã€‚",
  "Append TAGs": "é™„åŠ æ¨™ç±¤",
  "Replace underscores in filenames with spaces": "å°‡æª”æ¡ˆåç¨±ä¸­çš„åº•ç·šæ›¿æ›ç‚ºç©ºæ ¼",
  "Tag subfolders images as well": "æ¨™è¨˜å­è³‡æ–™å¤¾ä¸­çš„åœ–ç‰‡",
  "Recursive": "éè¿´æ¨™è¨˜",
  "Debug while tagging, it will print your image file with general tags and character tags.": "æ¨™è¨˜æ™‚é™¤éŒ¯ï¼Œå®ƒæœƒåˆ—å°å‡ºä½ çš„åœ–ç‰‡æª”æ¡ˆèˆ‡ä¸€èˆ¬æ¨™ç±¤å’Œè§’è‰²æ¨™ç±¤ã€‚",
  "Verbose logging": "è©³ç´°è¨˜éŒ„",
  "Show frequency of tags for images.": "é¡¯ç¤ºåœ–ç‰‡çš„æ¨™ç±¤é »ç‡ã€‚",
  "Show tags frequency": "é¡¯ç¤ºæ¨™ç±¤é »ç‡",
  "Model": "æ¨¡å‹",
  "Usefull to force model re download when switching to onnx": "åˆ‡æ›åˆ° onnx æ™‚ï¼Œå¼·åˆ¶é‡æ–°ä¸‹è¼‰æ¨¡å‹",
  "Force model re-download": "å¼·åˆ¶é‡æ–°ä¸‹è¼‰æ¨¡å‹",
  "General threshold": "ä¸€èˆ¬é–¾å€¼",
  "Adjust `general_threshold` for pruning tags (less tags, less flexible)": "èª¿æ•´ `general_threshold` ä»¥ä¿®å‰ªæ¨™ç±¤ (æ¨™ç±¤è¶Šå°‘ï¼Œå½ˆæ€§è¶Šå°)",
  "Character threshold": "è§’è‰²é–¾å€¼",
  "useful if you want to train with character": "å¦‚æœä½ æƒ³è¦ä½¿ç”¨è§’è‰²è¨“ç·´ï¼Œé€™å¾ˆæœ‰ç”¨",
  "Max dataloader workers": "æœ€å¤§è³‡æ–™è¼‰å…¥å·¥ä½œæ•¸",
  "Comma separated list of tags": "é€—è™Ÿåˆ†éš”çš„æ¨™ç±¤åˆ—è¡¨",
  "Load ğŸ’¾": "è®€å– ğŸ’¾",
  "Import ğŸ“„": "åŒ¯å…¥ ğŸ“„",
  "Options": "é¸é …",
  "Caption Separator": "æ¨™è¨˜æ–‡å­—åˆ†éš”ç¬¦è™Ÿ",
  "VAE batch size": "VAE æ‰¹æ¬¡å¤§å°",
  "Max grad norm": "æœ€å¤§æ¢¯åº¦è¦ç¯„ (Max grad norm)",
  "Learning rate Unet": "Unet å­¸ç¿’ç‡",
  "Set to 0 to not train the Unet": "è¨­ç‚º 0 ä»¥ä¸è¨“ç·´ Unet",
  "Learning rate TE": "æ–‡å­—ç·¨ç¢¼å™¨å­¸ç¿’ç‡",
  "Set to 0 to not train the Text Encoder": "è¨­ç‚º 0 ä»¥ä¸è¨“ç·´æ–‡å­—ç·¨ç¢¼å™¨",
  "Tools": "å·¥å…·",
  "Convert to LCM": "è½‰æ›æ¨¡å‹åˆ° LCM",
  "This utility convert a model to an LCM model.": "æ­¤å·¥å…·å°‡æ¨¡å‹è½‰æ›ç‚º LCM æ¨¡å‹ã€‚",
  "Stable Diffusion model to convert to LCM": "è¦è½‰æ›ç‚º LCM çš„ç©©å®šæ“´æ•£æ¨¡å‹",
  "Name of the new LCM model": "æ–° LCM æ¨¡å‹çš„åç¨±",
  "Path to the LCM file to create": "è¦å»ºç«‹çš„ LCM æª”æ¡ˆçš„è·¯å¾‘",
  "type the configuration file path or use the 'Open' button above to select it...": "è¼¸å…¥è¨­å®šæª”æ¡ˆçš„è·¯å¾‘ï¼Œæˆ–ä½¿ç”¨ä¸Šæ–¹çš„ã€ŒOpen ğŸ“‚ã€æŒ‰éˆ•ä¾†é¸æ“‡å®ƒ...",
  "Adjusts the scale of the rank dropout to maintain the average dropout rate, ensuring more consistent regularization across different layers.": "èª¿æ•´ç¶­åº¦ (Rank) æ¨æ£„çš„æ¯”ä¾‹ï¼Œä»¥ç¶­æŒå¹³å‡æ¨æ£„ç‡ï¼Œç¢ºä¿åœ¨ä¸åŒå±¤ä¹‹é–“æ›´ä¸€è‡´çš„æ­£è¦åŒ–ã€‚",
  "Rank Dropout Scale": "ç¶­åº¦ (Rank) æ¨æ£„æ¯”ä¾‹",
  "Selects trainable layers in a network, but trains normalization layers identically across methods as they lack matrix decomposition.": "é¸æ“‡ç¶²è·¯ä¸­å¯è¨“ç·´çš„å±¤ï¼Œä½†ç”±æ–¼ç¼ºä¹çŸ©é™£åˆ†è§£ï¼Œå› æ­¤ä»¥ç›¸åŒçš„æ–¹å¼è¨“ç·´æ­£è¦åŒ–å±¤ã€‚",
  "Train Norm": "è¨“ç·´æ­£è¦åŒ–",
  "LyCORIS Preset": "LyCORIS é è¨­ç¯„æœ¬",
  "Presets": "é è¨­ç¯„æœ¬",
  "Efficiently decompose tensor shapes, resulting in a sequence of convolution layers with varying dimensions and Hadamard product implementation through multiplication of two distinct tensors.": "æœ‰æ•ˆåœ°åˆ†è§£å¼µé‡å½¢ç‹€ï¼Œå¾è€Œç”¢ç”Ÿä¸€ç³»åˆ—å…·æœ‰ä¸åŒç¶­åº¦çš„å·ç©å±¤ï¼Œä¸¦é€šéå…©å€‹ä¸åŒå¼µé‡çš„ä¹˜æ³•å¯¦ç¾ Hadamard ä¹˜ç©ã€‚",
  "Use Tucker decomposition": "ä½¿ç”¨ Tucker åˆ†è§£",
  "Train an additional scalar in front of the weight difference, use a different weight initialization strategy.": "åœ¨æ¬Šé‡å·®ç•°å‰è¨“ç·´é¡å¤–çš„æ¨™é‡ï¼Œä½¿ç”¨ä¸åŒçš„æ¬Šé‡åˆå§‹åŒ–ç­–ç•¥ã€‚",
  "Use Scalar": "ä½¿ç”¨æ¨™é‡",
  "applies an additional scaling factor to the oft_blocks, allowing for further adjustment of their impact on the model's transformations.": "å° oft_blocks æ‡‰ç”¨é¡å¤–çš„ç¸®æ”¾å› å­ï¼Œå¾è€Œé€²ä¸€æ­¥èª¿æ•´å…¶å°æ¨¡å‹è½‰æ›çš„å½±éŸ¿ã€‚",
  "Rescaled OFT": "é‡æ–°ç¸®æ”¾ OFT",
  "Constrain OFT": "é™åˆ¶ OFT",
  "Limits the norm of the oft_blocks, ensuring that their magnitude does not exceed a specified threshold, thus controlling the extent of the transformation applied.": "é™åˆ¶ oft_blocks çš„è¦ç¯„ï¼Œç¢ºä¿å…¶å¤§å°ä¸è¶…éæŒ‡å®šçš„é–¾å€¼ï¼Œå¾è€Œæ§åˆ¶æ‡‰ç”¨çš„è½‰æ›ç¨‹åº¦ã€‚",
  "LoKr factor": "LoKr å› å­",
  "Set if we change the information going into the system (True) or the information coming out of it (False).": "é¸ç”¨å¾Œæœƒæ”¹è®Šé€²å…¥ç³»çµ±çš„è¨“ç·´è³‡æ–™ï¼Œè‹¥ä¸é¸å‰‡æœƒæ”¹è®Šè¼¸å‡ºç³»çµ±çš„è¨“ç·´è³‡æ–™ã€‚",
  "iA3 train on input": "iA3 è¨“ç·´è¼¸å…¥",
  "Controls whether both input and output dimensions of the layer's weights are decomposed into smaller matrices for reparameterization.": "æ§åˆ¶å±¤æ¬Šé‡çš„è¼¸å…¥å’Œè¼¸å‡ºç¶­åº¦æ˜¯å¦è¢«åˆ†è§£ç‚ºè¼ƒå°çš„çŸ©é™£ä»¥é€²è¡Œé‡æ–°åƒæ•¸åŒ–ã€‚",
  "LoKr decompose both": "LoKr åŒæ™‚åˆ†è§£",
  "Strength of the LCM": "LCM çš„å¼·åº¦",
  "folder where the training configuration files will be saved": "è¨“ç·´è¨­å®šæª”æ¡ˆå°‡æœƒè¢«å„²å­˜çš„è³‡æ–™å¤¾è·¯å¾‘",
  "folder where the training images are located": "è¨“ç·´åœ–ç‰‡çš„è³‡æ–™å¤¾è·¯å¾‘",
  "folder where the model will be saved": "æ¨¡å‹å°‡æœƒè¢«å„²å­˜çš„è³‡æ–™å¤¾è·¯å¾‘",
  "Model type": "æ¨¡å‹é¡å‹",
  "Extract LCM": "æå– LCM",
  "Verfiy LoRA": "é©—è­‰ LoRA",
  "Path to an existing LoRA network weights to resume training from": "è¦å¾ä¸­ç¹¼çºŒè¨“ç·´çš„ç¾æœ‰ LoRA ç¶²è·¯æ¬Šé‡çš„è·¯å¾‘",
  "Seed": "ç¨®å­",
  "(Optional) eg:1234": " (é¸å¡«) ä¾‹å¦‚ï¼š1234",
  "(Optional) eg: \"milestones=[1,10,30,50]\" \"gamma=0.1\"": " (é¸å¡«) ä¾‹å¦‚ï¼š \"milestones=[1,10,30,50]\" \"gamma=0.1\"",
  "(Optional) eg: relative_step=True scale_parameter=True warmup_init=True": " (é¸å¡«) ä¾‹å¦‚ï¼šrelative_step=True scale_parameter=True warmup_init=True",
  "(Optional) For Cosine with restart and polynomial only": " (é¸å¡«) åªé©ç”¨æ–¼é¤˜å¼¦å‡½æ•¸ä¸¦ä½¿ç”¨é‡å•Ÿ (cosine_with_restart) å’Œå¤šé …å¼ (polynomial)",
  "Network Rank (Dimension)": "ç¶²è·¯ç¶­åº¦ (Rank)",
  "Network Alpha": "ç¶²è·¯ Alpha",
  "alpha for LoRA weight scaling": "LoRA æ¬Šé‡ç¸®æ”¾çš„ Alpha å€¼",
  "Convolution Rank (Dimension)": "å·ç©ç¶­åº¦ (Rank)",
  "Convolution Alpha": "å·ç© Alpha",
  "Max Norm Regularization is a technique to stabilize network training by limiting the norm of network weights. It may be effective in suppressing overfitting of LoRA and improving stability when used with other LoRAs. See PR #545 on kohya_ss/sd_scripts repo for details. Recommended setting: 1. Higher is weaker, lower is stronger.": "æœ€å¤§è¦ç¯„æ­£è¦åŒ–æ˜¯ä¸€ç¨®ç©©å®šç¶²è·¯è¨“ç·´çš„æŠ€è¡“ï¼Œé€šéé™åˆ¶ç¶²è·¯æ¬Šé‡çš„è¦ç¯„ä¾†å¯¦ç¾ã€‚ç•¶èˆ‡å…¶ä»– LoRA ä¸€èµ·ä½¿ç”¨æ™‚ï¼Œå®ƒå¯èƒ½æœƒæœ‰æ•ˆåœ°æŠ‘åˆ¶ LoRA çš„éåº¦æ“¬åˆä¸¦æé«˜ç©©å®šæ€§ã€‚è©³ç´°è³‡æ–™è«‹è¦‹ kohya_ss/sd_scripts Github ä¸Šçš„ PR#545ã€‚å»ºè­°è¨­ç½®ï¼š1.0 è¶Šé«˜è¶Šå¼±ï¼Œè¶Šä½è¶Šå¼·ã€‚",
  "Is a normal probability dropout at the neuron level. In the case of LoRA, it is applied to the output of down. Recommended range 0.1 to 0.5": "æ˜¯ç¥ç¶“å…ƒç´šçš„æ­£å¸¸æ¦‚ç‡æ¨æ£„ã€‚åœ¨ LoRA çš„æƒ…æ³ä¸‹ï¼Œå®ƒè¢«æ‡‰ç”¨æ–¼ Down Sampler çš„è¼¸å‡ºã€‚å»ºè­°ç¯„åœ 0.1 åˆ° 0.5",
  "can specify `rank_dropout` to dropout each rank with specified probability. Recommended range 0.1 to 0.3": "å¯ä»¥æŒ‡å®š `rank_dropout` ä»¥æŒ‡å®šçš„æ¦‚ç‡æ¨æ£„æ¯å€‹ç¶­åº¦ã€‚å»ºè­°ç¯„åœ 0.1 åˆ° 0.3",
  "can specify `module_dropout` to dropout each rank with specified probability. Recommended range 0.1 to 0.3": "å¯ä»¥æŒ‡å®š `module_dropout` ä»¥æŒ‡å®šçš„æ¦‚ç‡æ¨æ£„æ¯å€‹ç¶­åº¦ã€‚å»ºè­°ç¯„åœ 0.1 åˆ° 0.3",
  "Folder where the training folders containing the images are located": "è¨“ç·´è³‡æ–™å¤¾çš„è³‡æ–™å¤¾è·¯å¾‘ï¼ŒåŒ…å«åœ–ç‰‡",
  "(Optional) Folder where where the regularization folders containing the images are located": " (é¸å¡«) æ­£è¦åŒ–è³‡æ–™å¤¾çš„è³‡æ–™å¤¾è·¯å¾‘ï¼ŒåŒ…å«åœ–ç‰‡",
  "Folder to output trained model": "è¼¸å‡ºè¨“ç·´æ¨¡å‹çš„è³‡æ–™å¤¾è·¯å¾‘",
  "Optional: enable logging and output TensorBoard log to this folder": "é¸å¡«ï¼šå•Ÿç”¨è¨˜éŒ„ä¸¦å°‡ TensorBoard è¨˜éŒ„è¼¸å‡ºåˆ°æ­¤è³‡æ–™å¤¾",
  "Pretrained model name or path": "é è¨“ç·´æ¨¡å‹åç¨±æˆ–è·¯å¾‘",
  "enter the path to custom model or name of pretrained model": "è¼¸å…¥è‡ªè¨‚æ¨¡å‹çš„è·¯å¾‘æˆ–é è¨“ç·´æ¨¡å‹çš„åç¨±",
  "(Name of the model to output)": " (è¼¸å‡ºçš„æ¨¡å‹åç¨±)",
  "LoRA type": "LoRA é¡å‹",
  "(Optional) path to checkpoint of vae to replace for training": " (é¸å¡«) è¦æ›¿æ›è¨“ç·´çš„ VAE checkpoint çš„è·¯å¾‘",
  "(Optional) Use to provide additional parameters not handled by the GUI. Eg: --some_parameters \"value\"": " (é¸å¡«) ç”¨æ–¼æä¾› GUI æœªè™•ç†çš„é¡å¤–åƒæ•¸ã€‚ä¾‹å¦‚ï¼š--some_parameters \"value\"",
  "Automates the processing of noise, allowing for faster model fitting, as well as balancing out color issues": "è‡ªå‹•è™•ç†å™ªè²ï¼Œå¯ä»¥æ›´å¿«åœ°æ“¬åˆæ¨¡å‹ï¼ŒåŒæ™‚å¹³è¡¡é¡è‰²å•é¡Œ",
  "Debiased Estimation loss": "åå·®ä¼°ç®—æå¤± (Debiased Estimation loss)",
  "(Optional) Override number of epoch. Default: 8": " (é¸å¡«) è¦†è“‹é€±æœŸ (Epoch) æ•¸é‡ã€‚é è¨­ï¼š8",
  "Weights": "æ¬Šé‡",
  "Down LR weights": "Down LR æ¬Šé‡",
  "Mid LR weights": "Mid LR æ¬Šé‡",
  "Up LR weights": "Up LR æ¬Šé‡",
  "Blocks LR zero threshold": "å€å¡Š LR é›¶é–¾å€¼",
  "(Optional) eg: 0,0,0,0,0,0,1,1,1,1,1,1": " (é¸å¡«) ä¾‹å¦‚ï¼š0,0,0,0,0,0,1,1,1,1,1,1",
  "(Optional) eg: 0.5": " (é¸å¡«) ä¾‹å¦‚ï¼š0.5",
  "(Optional) eg: 0.1": " (é¸å¡«) ä¾‹å¦‚ï¼š0.1",
  "Specify the learning rate weight of the down blocks of U-Net.": "æŒ‡å®š U-Net ä¸‹å€å¡Šçš„å­¸ç¿’ç‡æ¬Šé‡ã€‚",
  "Specify the learning rate weight of the mid block of U-Net.": "æŒ‡å®š U-Net ä¸­å€å¡Šçš„å­¸ç¿’ç‡æ¬Šé‡ã€‚",
  "Specify the learning rate weight of the up blocks of U-Net. The same as down_lr_weight.": "æŒ‡å®š U-Net ä¸Šå€å¡Šçš„å­¸ç¿’ç‡æ¬Šé‡ã€‚èˆ‡ down_lr_weight ç›¸åŒã€‚",
  "If the weight is not more than this value, the LoRA module is not created. The default is 0.": "å¦‚æœæ¬Šé‡ä¸è¶…éæ­¤å€¼ï¼Œå‰‡ä¸æœƒå‰µå»º LoRA æ¨¡çµ„ã€‚é è¨­ç‚º 0ã€‚",
  "Blocks": "å€å¡Š",
  "Block dims": "å€å¡Šç¶­åº¦",
  "(Optional) eg: 2,2,2,2,4,4,4,4,6,6,6,6,8,6,6,6,6,4,4,4,4,2,2,2,2": " (é¸å¡«) ä¾‹å¦‚ï¼š2,2,2,2,4,4,4,4,6,6,6,6,8,6,6,6,6,4,4,4,4,2,2,2,2",
  "Specify the dim (rank) of each block. Specify 25 numbers.": "æŒ‡å®šæ¯å€‹å€å¡Šçš„ç¶­åº¦ (Rank)ã€‚æŒ‡å®š 25 å€‹æ•¸å­—ã€‚",
  "Specify the alpha of each block. Specify 25 numbers as with block_dims. If omitted, the value of network_alpha is used.": "æŒ‡å®šæ¯å€‹å€å¡Šçš„ Alphaã€‚èˆ‡å€å¡Šç¶­åº¦ä¸€æ¨£ï¼ŒæŒ‡å®š 25 å€‹æ•¸å­—ã€‚å¦‚æœçœç•¥ï¼Œå‰‡ä½¿ç”¨ç¶²è·¯ Alpha çš„å€¼ã€‚",
  "Conv": "å·ç©",
  "Conv dims": "å·ç©ç¶­åº¦ (dims)",
  "Conv alphas": "å·ç© Alphas",
  "Extend LoRA to Conv2d 3x3 and specify the dim (rank) of each block. Specify 25 numbers.": "å°‡ LoRA æ“´å±•åˆ° Conv2d 3x3ï¼Œä¸¦æŒ‡å®šæ¯å€‹å€å¡Šçš„ç¶­åº¦ (Rank)ã€‚æŒ‡å®š 25 å€‹æ•¸å­—ã€‚",
  "Specify the alpha of each block when expanding LoRA to Conv2d 3x3. Specify 25 numbers. If omitted, the value of conv_alpha is used.": "å°‡ LoRA æ“´å±•åˆ° Conv2d 3x3 æ™‚ï¼ŒæŒ‡å®šæ¯å€‹å€å¡Šçš„ Alphaã€‚æŒ‡å®š 25 å€‹æ•¸å­—ã€‚å¦‚æœçœç•¥ï¼Œå‰‡ä½¿ç”¨å·ç© Alpha çš„å€¼ã€‚",
  "Weighted captions": "åŠ æ¬Šæ¨™è¨˜æ–‡å­—",
  "About SDXL training": "é—œæ–¼ SDXL è¨“ç·´",
  "Adaptive noise scale": "è‡ªé©æ‡‰å™ªè²æ¯”ä¾‹",
  "Additional parameters": "é¡å¤–åƒæ•¸",
  "Advanced options": "é€²éšé¸é …",
  "Advanced parameters": "é€²éšåƒæ•¸",
  "Advanced": "é€²éš",
  "ashleykleynhans runpod docker builds": "ashleykleynhans runpod docker å»ºæ§‹",
  "Automatically determine the dim(rank) from the weight file.": "å¾æŒ‡å®šçš„æ¬Šé‡æª”æ¡ˆè‡ªå‹•æ±ºå®š dim(rank)ã€‚",
  "Autosave": "è‡ªå‹•å„²å­˜",
  "Basic Captioning": "åŸºæœ¬æ¨™è¨˜",
  "Basic": "åŸºæœ¬",
  "Batch size": "æ‰¹æ¬¡å¤§å°",
  "BLIP Captioning": "BLIP æ¨™è¨˜",
  "Bucket resolution steps": "åˆ†æ¡¶è§£æåº¦é–“éš”",
  "Built with Gradio": "ä½¿ç”¨ Gradio å»ºæ§‹",
  "Cache latents to disk": "æš«å­˜æ½›ç©ºé–“è³‡æ–™åˆ°ç¡¬ç¢Ÿ",
  "Cache latents": "æš«å­˜æ½›ç©ºé–“è³‡æ–™",
  "Caption file extension": "æ¨™è¨˜æª”æ¡ˆå‰¯æª”å",
  "Caption Extension": "æ¨™è¨˜æª”æ¡ˆå‰¯æª”å",
  "(Optional) Extension for caption files. default: .caption": " (é¸å¡«) æ¨™è¨˜æª”æ¡ˆçš„å‰¯æª”åã€‚é è¨­ï¼š.caption",
  "Caption text": "æ¨™è¨˜æ–‡å­—",
  "caption": "æ¨™è¨˜",
  "Change History": "è®Šæ›´è¨˜éŒ„",
  "Class prompt": "é¡ (Class) æè©",
  "Color augmentation": "é¡è‰²å¢å¼·",
  "Configuration file": "è¨­å®šæª”",
  "constant_with_warmup": "å¸¸æ•¸ä¸¦ä½¿ç”¨é ç†± (constant_with_warmup)",
  "constant": "å¸¸æ•¸ (constant)",
  "Conv Dimension (Rank)": "å·ç©ç¶­åº¦ (Rank)",
  "Conv Dimension": "å·ç©ç¶­åº¦",
  "Convert model": "è½‰æ›æ¨¡å‹",
  "Copy info to Folders Tab": "è¤‡è£½è³‡è¨Šåˆ°è³‡æ–™å¤¾é ç±¤",
  "cosine_with_restarts": "é¤˜å¼¦å‡½æ•¸ä¸¦ä½¿ç”¨é‡å•Ÿ",
  "cosine": "é¤˜å¼¦å‡½æ•¸ (cosine)",
  "CrossAttention": "äº¤å‰æ³¨æ„åŠ›",
  "DANGER!!! -- Insecure folder renaming -- DANGER!!!": "å±éšªï¼ï¼ï¼ -- ä¸å®‰å…¨çš„è³‡æ–™å¤¾é‡æ–°å‘½å -- å±éšªï¼ï¼ï¼",
  "Dataset folder": "è³‡æ–™é›†è³‡æ–™å¤¾",
  "Dataset preparation": "è³‡æ–™é›†æº–å‚™",
  "Dataset Preparation": "è³‡æ–™é›†æº–å‚™",
  "Dataset repeats": "è³‡æ–™é›†é‡è¤‡æ•¸",
  "Desired LoRA rank": "å¸Œæœ› LoRA çš„ç¶­åº¦ (Rank)",
  "Destination training directory": "è¨“ç·´çµæœè³‡æ–™å¤¾",
  "Device": "è£ç½®",
  "DIM from weights": "å¾æ¬Šé‡è®€å– DIM",
  "Directory containing the images to caption": "å«æœ‰éœ€æ¨™è¨˜çš„åœ–ç‰‡è³‡æ–™å¤¾",
  "Directory containing the training images": "è¨“ç·´çš„åœ–ç‰‡è³‡æ–™å¤¾",
  "Directory where formatted training and regularisation folders will be placed": "è¨“ç·´èˆ‡æ­£è¦åŒ–è³‡æ–™å¤¾å°‡æœƒè¢«å–ä»£",
  "Disable CP decomposition": "åœç”¨ CP åˆ†è§£æ³•",
  "Do not copy other files in the input folder to the output folder": "ä¸è¦å°‡è¼¸å…¥è³‡æ–™å¤¾ä¸­çš„å…¶ä»–æª”æ¡ˆï¼Œè¤‡è£½åˆ°è¼¸å‡ºè³‡æ–™å¤¾",
  "Do not copy other files": "ä¸è¤‡è£½å…¶ä»–æª”æ¡ˆ",
  "Don't upscale bucket resolution": "ä¸è¦æ”¾å¤§åˆ†æ¡¶è§£æåº¦",
  "Dreambooth/LoRA Dataset balancing": "Dreambooth/LoRA è³‡æ–™é›†å¹³è¡¡",
  "Dreambooth/LoRA Folder preparation": "Dreambooth/LoRA æº–å‚™è³‡æ–™å¤¾",
  "Dropout caption every n epochs": "åœ¨æ¯ N å€‹é€±æœŸ (Epoch) ä¸Ÿæ£„æ¨™è¨˜",
  "DyLoRA model": "DyLoRA æ¨¡å‹",
  "Dynamic method": "å£“ç¸®æ¼”ç®—æ³•",
  "Dynamic parameter": "å£“ç¸®åƒæ•¸",
  "e.g., \"by some artist\". Leave empty if you only want to add a prefix or postfix.": "ä¾‹å¦‚ï¼Œ\"ç”±æŸå€‹è—è¡“å®¶å‰µä½œ\"ã€‚å¦‚æœä½ åªæƒ³åŠ å…¥å‰ç¶´æˆ–å¾Œç¶´ï¼Œè«‹ç•™ç©ºç™½ã€‚",
  "e.g., \"by some artist\". Leave empty if you want to replace with nothing.": "ä¾‹å¦‚ï¼Œ\"ç”±æŸå€‹è—è¡“å®¶å‰µä½œ\"ã€‚å¦‚æœä½ æƒ³ç”¨ç©ºå€¼å–ä»£ï¼Œè«‹ç•™ç©ºç™½ã€‚",
  "Enable buckets": "å•Ÿç”¨è³‡æ–™æ¡¶",
  "Enable for Hugging Face's stabilityai models": "å•Ÿç”¨ HuggingFace çš„ stabilityai æ¨¡å‹",
  "Enter one sample prompt per line to generate multiple samples per cycle. Optional specifiers include: --w (width), --h (height), --d (seed), --l (cfg scale), --s (sampler steps) and --n (negative prompt). To modify sample prompts during training, edit the prompt.txt file in the samples directory.": "æ¯è¡Œè¼¸å…¥ä¸€å€‹æç¤ºè©ä¾†ç”Ÿæˆæ¯å€‹è¨“ç·´é€±æœŸçš„è¼¸å‡ºç¯„æœ¬ã€‚å¯ä»¥é¸æ“‡æŒ‡å®šçš„åƒæ•¸ï¼ŒåŒ…æ‹¬ï¼š--w (å¯¬åº¦) ï¼Œ--h (é«˜åº¦) ï¼Œ--d (ç¨®å­) ï¼Œ--l (CFG æ¯”ä¾‹) ï¼Œ--s (æ¡æ¨£å™¨æ­¥é©Ÿ) å’Œ --n (è² é¢æç¤ºè©) ã€‚å¦‚æœè¦åœ¨è¨“ç·´é€±æœŸä¸­ä¿®æ”¹æç¤ºè©ï¼Œè«‹ä¿®æ”¹ç¯„æœ¬ç›®éŒ„ä¸­çš„ prompt.txt æª”æ¡ˆã€‚",
  "Epoch": "é€±æœŸ (Epoch)",
  "Error": "éŒ¯èª¤",
  "Example of the optimizer settings for Adafactor with the fixed learning rate:": "å›ºå®šå­¸ç¿’ç‡ Adafactor å„ªåŒ–å™¨çš„è¨­å®šç¯„ä¾‹ï¼š",
  "Extract DyLoRA": "æå– DyLoRA",
  "Extract LoRA model": "æå– LoRAæ¨¡å‹",
  "Extract LoRA": "æå– LoRA",
  "Extract LyCORIS LoCon": "æå– LyCORIS LoCon",
  "Extract LyCORIS LoCON": "æå– LyCORIS LoCON",
  "FileNotFoundError": "éŒ¯èª¤ï¼æª”æ¡ˆæ‰¾ä¸åˆ°",
  "Find text": "å°‹æ‰¾æ–‡å­—",
  "Finetune": "å¾®èª¿",
  "Finetuned model": "å¾®èª¿æ¨¡å‹",
  "Finetuning Resource Guide": "å¾®èª¿è³‡æºæŒ‡å—",
  "fixed": "å›ºå®š",
  "Flip augmentation": "ç¿»è½‰å¢å¼·",
  "float16": "float16",
  "Folders": "è³‡æ–™å¤¾",
  "U-Net and Text Encoder can be trained with fp8 (experimental)": "U-Net èˆ‡ Text Encoder å¯ä»¥ä½¿ç”¨ fp8 è¨“ç·´ (å¯¦é©—æ€§åŠŸèƒ½)",
  "fp8 base training (experimental)": "ä½¿ç”¨ fp8 åŸºç¤è¨“ç·´ (å¯¦é©—æ€§åŠŸèƒ½)",
  "Full bf16 training (experimental)": "å®Œæ•´ä½¿ç”¨ bf16 è¨“ç·´ (å¯¦é©—æ€§åŠŸèƒ½)",
  "Full fp16 training (experimental)": "å®Œæ•´ä½¿ç”¨ fp16 è¨“ç·´ (å¯¦é©—æ€§åŠŸèƒ½)",
  "Generate caption files for the grouped images based on their folder name": "æ ¹æ“šåœ–ç‰‡çš„è³‡æ–™å¤¾åç¨±ç”Ÿæˆæ¨™è¨˜æ–‡å­—æª”æ¡ˆ",
  "Generate caption metadata": "ç”Ÿæˆæ¨™è¨˜æ–‡å­—å¾Œè¨­è³‡æ–™",
  "Generate Captions": "ç”Ÿæˆæ¨™è¨˜æ–‡å­—",
  "Generate image buckets metadata": "ç”Ÿæˆåœ–åƒåˆ†æ¡¶å¾Œè¨­è³‡æ–™",
  "GIT Captioning": "GIT æ¨™è¨˜æ–‡å­—",
  "Gradient accumulate steps": "æ¢¯åº¦ç´¯åŠ æ­¥æ•¸",
  "Gradient checkpointing": "æ¢¯åº¦æª¢æŸ¥é»",
  "Group size": "ç¾¤çµ„å¤§å°",
  "Guidelines for SDXL Finetuning": "SDXL å¾®èª¿æŒ‡å—",
  "Guides": "æŒ‡å—",
  "How to Create a LoRA Part 1: Dataset Preparation:": "å¦‚ä½•å»ºç«‹ LoRA ç¬¬ 1 éƒ¨ä»½ï¼šè³‡æ–™é›†æº–å‚™ï¼š",
  "If unchecked, tensorboard will be used as the default for logging.": "å¦‚æœä¸å‹¾é¸ï¼ŒTensorboard å°‡æœƒä½¿ç”¨é è¨­çš„ç´€éŒ„æ–¹å¼ã€‚",
  "If you have valuable resources to add, kindly create a PR on Github.": "å¦‚æœä½ æœ‰æœ‰åƒ¹å€¼çš„è³‡æºè¦å¢åŠ ï¼Œè«‹åœ¨ Github ä¸Šå»ºç«‹ä¸€å€‹ PRã€‚",
  "Ignore Imported Tags Above Word Count": "ç•¥éé«˜æ–¼å­—æ•¸æ•¸é‡çš„æ¨™è¨˜æ¨™ç±¤",
  "Image folder to caption": "è¦åŠ å…¥æ¨™è¨˜çš„åœ–ç‰‡è³‡æ–™å¤¾",
  "Image folder": "åœ–ç‰‡è³‡æ–™å¤¾",
  "Include images in subfolders as well": "åŒ…å«å­è³‡æ–™å¤¾ä¸­çš„åœ–ç‰‡",
  "Include Subfolders": "åŒ…å«å­è³‡æ–™å¤¾",
  "Init word": "åˆå§‹åŒ–æ¨™è¨˜æ–‡å­—",
  "Input folder": "è¼¸å…¥è³‡æ–™å¤¾",
  "Install Location": "å®‰è£ä½ç½®",
  "Installation": "å®‰è£",
  "Instance prompt": "å¯¦ä¾‹ (Instance) æç¤ºè©",
  "Keep n tokens": "ä¿ç•™ N å€‹æç¤ºè©",
  "Launching the GUI on Linux and macOS": "åœ¨ Linux/macOS ä¸Šå•Ÿå‹• GUI",
  "Launching the GUI on Windows": "åœ¨ Windows ä¸Šå•Ÿå‹• GUI",
  "Learning rate": "å­¸ç¿’ç‡",
  "adafactor": "è‡ªé©æ‡‰å­¸ç¿’ (adafactor)",
  "linear": "ç·šæ€§ (linear)",
  "Linux and macOS Upgrade": "Linux/macOS å‡ç´š",
  "Linux and macOS": "Linux/macOS",
  "Linux Pre-requirements": "Linux Pre-requirements",
  "Load": "è¼‰å…¥",
  "Loading...": "è¼‰å…¥ä¸­...",
  "Local docker build": "Docker å»ºæ§‹",
  "Logging folder": "è¨˜éŒ„è³‡æ–™å¤¾",
  "LoRA model \"A\"": "LoRA æ¨¡å‹ \"A\"",
  "LoRA model \"B\"": "LoRA æ¨¡å‹ \"B\"",
  "LoRA model \"C\"": "LoRA æ¨¡å‹ \"C\"",
  "LoRA model \"D\"": "LoRA æ¨¡å‹ \"D\"",
  "LoRA model": "LoRA æ¨¡å‹",
  "LoRA network weights": "LoRA ç¶²è·¯æ¬Šé‡",
  "LoRA": "LoRA",
  "LR number of cycles": "å­¸ç¿’ç‡é€±æœŸæ•¸",
  "LR power": "å­¸ç¿’ç‡ä¹˜å†ª",
  "LR scheduler extra arguments": "å­¸ç¿’ç‡èª¿åº¦å™¨é¡å¤–åƒæ•¸",
  "LR Scheduler": "å­¸ç¿’ç‡èª¿åº¦å™¨",
  "LR warmup (% of steps)": "å­¸ç¿’ç‡é ç†± (% çš„æ­¥æ•¸)",
  "LyCORIS model": "LyCORIS æ¨¡å‹",
  "Macos is not great at the moment.": "ç›®å‰ MacOS æ”¯æ´ä¸¦ä¸æ˜¯å¾ˆå¥½ã€‚",
  "Manual Captioning": "æ‰‹å‹•æ¨™è¨˜æ–‡å­—",
  "Manual installation": "æ‰‹å‹•å®‰è£",
  "Max bucket resolution": "æœ€å¤§è³‡æ–™å„²å­˜æ¡¶è§£æåº¦",
  "Max length": "æœ€å¤§é•·åº¦",
  "Max num workers for DataLoader": "è³‡æ–™å·¥ä½œè¼‰å…¥çš„æœ€å¤§å·¥ä½œæ•¸é‡",
  "Max resolution": "æœ€å¤§è§£æåº¦",
  "Max Timestep": "æœ€å¤§æ™‚åºæ­¥æ•¸",
  "Max Token Length": "æœ€å¤§æ¨™è¨˜é•·åº¦",
  "Max train epoch": "æœ€å¤§è¨“ç·´é€±æœŸ (Epoch) æ•¸",
  "Max train steps": "æœ€å¤§è¨“ç·´ç¸½æ­¥æ•¸",
  "Maximum bucket resolution": "æœ€å¤§è³‡æ–™å„²å­˜æ¡¶è§£æåº¦",
  "Maximum size in pixel a bucket can be (>= 64)": "æœ€å¤§è³‡æ–™å„²å­˜æ¡¶è§£æåº¦å¯é” (>= 64) ",
  "Memory efficient attention": "é«˜æ•ˆè¨˜æ†¶é«”æ³¨æ„åŠ›å€å¡Šè™•ç†",
  "Merge LoRA (SVD)": "åˆä½µ LoRA (SVD) ",
  "Merge LoRA": "åˆä½µ LoRA",
  "Merge LyCORIS": "åˆä½µ LyCORIS",
  "Merge model": "åˆä½µæ¨¡å‹",
  "Merge precision": "åˆä½µç²¾åº¦",
  "Merge ratio model A": "æ¨¡å‹ A åˆä½µæ¯”ä¾‹",
  "Merge ratio model B": "æ¨¡å‹ B åˆä½µæ¯”ä¾‹",
  "Merge ratio model C": "æ¨¡å‹ C åˆä½µæ¯”ä¾‹",
  "Merge ratio model D": "æ¨¡å‹ D åˆä½µæ¯”ä¾‹",
  "Min bucket resolution": "æœ€å°è³‡æ–™å„²å­˜æ¡¶è§£æåº¦",
  "Min length": "æœ€å°é•·åº¦",
  "Min SNR gamma": "æœ€å° SNR gamma",
  "Min Timestep": "æœ€å°æ™‚åºæ­¥æ•¸",
  "Minimum bucket resolution": "æœ€å°è³‡æ–™å„²å­˜æ¡¶è§£æåº¦",
  "Minimum size in pixel a bucket can be (>= 64)": "æœ€å°è³‡æ–™å„²å­˜æ¡¶è§£æåº¦å¯é” (>= 64) ",
  "Mixed precision": "æ··åˆç²¾åº¦",
  "Mnimum difference": "æœ€å°åŒ–å·®ç•°",
  "Mode": "æ¨¡å¼",
  "Model A merge ratio (eg: 0.5 mean 50%)": "æ¨¡å‹ A åˆä½µæ¯”ç‡ (ä¾‹å¦‚ï¼š0.5 æŒ‡çš„æ˜¯ 50%) ",
  "Model B merge ratio (eg: 0.5 mean 50%)": "æ¨¡å‹ B åˆä½µæ¯”ç‡ (ä¾‹å¦‚ï¼š0.5 æŒ‡çš„æ˜¯ 50%) ",
  "Model C merge ratio (eg: 0.5 mean 50%)": "æ¨¡å‹ C åˆä½µæ¯”ç‡ (ä¾‹å¦‚ï¼š0.5 æŒ‡çš„æ˜¯ 50%) ",
  "Model D merge ratio (eg: 0.5 mean 50%)": "æ¨¡å‹ D åˆä½µæ¯”ç‡ (ä¾‹å¦‚ï¼š0.5 æŒ‡çš„æ˜¯ 50%) ",
  "Model output folder": "æ¨¡å‹è¼¸å‡ºè³‡æ–™å¤¾",
  "Model output name": "æ¨¡å‹è¼¸å‡ºè³‡æ–™å¤¾",
  "Model Quick Pick": "å¿«é€Ÿé¸æ“‡æ¨¡å‹",
  "Module dropout": "æ¨¡å‹æ¨æ£„",
  "Network Dimension (Rank)": "ç¶²è·¯ç¶­åº¦ (Rank)",
  "Network Dimension": "ç¶²è·¯ç¶­åº¦",
  "Network dropout": "ç¶²è·¯æ¨æ£„",
  "No module called tkinter": "æ²’æœ‰åç¨±ç‚º tkinter çš„æ¨¡çµ„",
  "No token padding": "ä¸åšæç¤ºè©å¡«å……",
  "Noise offset type": "å™ªè²åç§»é¡å‹",
  "Noise offset": "å™ªè²åç§»",
  "Number of beams": "beam çš„æ•¸é‡",
  "Number of CPU threads per core": "æ¯å€‹ CPU æ ¸å¿ƒçš„ç·šç¨‹æ•¸",
  "Number of images to group together": "è¦ä¸€èµ·åˆ†çµ„çš„åœ–ç‰‡æ•¸é‡",
  "Number of updates steps to accumulate before performing a backward/update pass": "åŸ·è¡Œåå‘/æ›´æ–°å‚³éä¹‹å‰ï¼Œéœ€è¦ç´¯ç©çš„æ›´æ–°æ­¥é©Ÿæ•¸",
  "object template": "ç‰©ä»¶æ¨£ç‰ˆ",
  "Only for SD v2 models. By scaling the loss according to the time step, the weights of global noise prediction and local noise prediction become the same, and the improvement of details may be expected.": "åƒ…é©ç”¨æ–¼ SD v2 æ¨¡å‹ã€‚é€šéæ ¹æ“šæ™‚åºæ­¥æ•¸çš„ç¸®æ”¾æå¤±ï¼Œæ•´é«”çš„å™ªè²é æ¸¬èˆ‡å±€éƒ¨çš„å™ªè²é æ¸¬çš„æ¬Šé‡æœƒè®Šå¾—ç›¸åŒï¼Œä»¥æ­¤å¸Œæœ›èƒ½æ”¹å–„ç´°ç¯€ã€‚",
  "Open": "é–‹å•Ÿ",
  "Optimizer extra arguments": "å„ªåŒ–å™¨é¡å¤–åƒæ•¸",
  "Optimizer": "å„ªåŒ–å™¨",
  "Optional: CUDNN 8.6": "å¯é¸ï¼šCUDNN 8.6",
  "Original": "åŸå§‹",
  "Output folder": "è¼¸å‡ºè³‡æ–™å¤¾",
  "Output": "è¼¸å‡º",
  "Overwrite existing captions in folder": "è¦†è“‹è³‡æ–™å¤¾ä¸­ç¾æœ‰çš„æç¤ºè©",
  "Page File Limit": "åˆ†é æª”æ¡ˆé™åˆ¶",
  "PagedAdamW8bit": "PagedAdamW8bit",
  "PagedLion8bit": "PagedLion8bit",
  "Parameters": "åƒæ•¸",
  "path for the checkpoint file to save...": "å„²å­˜ checkpoint æª”æ¡ˆè·¯å¾‘...",
  "path for the LoRA file to save...": "å„²å­˜ LoRA æª”æ¡ˆè·¯å¾‘...",
  "path for the new LoRA file to save...": "å„²å­˜æ–° LoRA æª”æ¡ˆè·¯å¾‘...",
  "path to \"last-state\" state folder to resume from": "ç”¨ä¾†ç¹¼çºŒè¨“ç·´çš„ \"æœ€å¾Œç‹€æ…‹\" è³‡æ–™å¤¾è·¯å¾‘",
  "Path to the DyLoRA model to extract from": "è¦æå– DyLoRA æ¨¡å‹çš„è·¯å¾‘",
  "Path to the finetuned model to extract": "è¦æå–çš„å¾®èª¿æ¨¡å‹çš„è·¯å¾‘",
  "Path to the LoRA A model": "LoRA A æ¨¡å‹çš„è·¯å¾‘",
  "Path to the LoRA B model": "LoRA B æ¨¡å‹çš„è·¯å¾‘",
  "Path to the LoRA C model": "LoRA C æ¨¡å‹çš„è·¯å¾‘",
  "Path to the LoRA D model": "LoRA D æ¨¡å‹çš„è·¯å¾‘",
  "Path to the LoRA model to verify": "è¦é©—è­‰çš„ LoRA æ¨¡å‹çš„è·¯å¾‘",
  "Path to the LoRA to resize": "è¦èª¿æ•´å¤§å°çš„ LoRA çš„è·¯å¾‘",
  "Path to the LyCORIS model": "LyCORIS æ¨¡å‹çš„è·¯å¾‘",
  "path where to save the extracted LoRA model...": "å„²å­˜æå–å‡ºçš„ LoRA æ¨¡å‹çš„è·¯å¾‘...",
  "Persistent data loader": "æŒçºŒè³‡æ–™è¼‰å…¥å™¨",
  "polynomial": "å¤šé …å¼ (polynomial)",
  "Postfix to add to BLIP caption": "æ·»åŠ åˆ° BLIP æç¤ºè©çš„å¾Œç¶´",
  "Postfix to add to caption": "æ·»åŠ åˆ°æç¤ºè©çš„å¾Œç¶´",
  "Pre-built Runpod template": "é å…ˆå»ºæ§‹çš„ Runpod æ¨£ç‰ˆ",
  "Prefix to add to BLIP caption": "æ·»åŠ åˆ° BLIP æç¤ºè©çš„å‰ç¶´",
  "Prefix to add to caption": "æ·»åŠ åˆ°æç¤ºè©çš„å‰ç¶´",
  "Prepare training data": "æº–å‚™è¨“ç·´è³‡æ–™é›†",
  "Print training command": "å°å‡ºè¨“ç·´æŒ‡ä»¤",
  "Prior loss weight": "æ­£è¦åŒ–é©—è­‰æå¤±æ¬Šé‡",
  "Prodigy": "Prodigy",
  "Provide a SD file path IF you want to merge it with LoRA files": "å¦‚æœä½ è¦åˆä½µ LoRA æª”æ¡ˆï¼Œè«‹æä¾› SD æª”æ¡ˆè³‡æ–™å¤¾è·¯å¾‘",
  "Provide a SD file path that you want to merge with the LyCORIS file": "è«‹æä¾›ä½ æƒ³è¦èˆ‡ LyCORIS æª”æ¡ˆåˆä½µçš„ SD æª”æ¡ˆè³‡æ–™å¤¾è·¯å¾‘",
  "PyTorch 2 seems to use slightly less GPU memory than PyTorch 1.": "PyTorch 2 ä¼¼ä¹ä½¿ç”¨çš„ GPU è¨˜æ†¶é«”æ¯” PyTorch 1 ç•¥å°‘ã€‚",
  "Quick Tags": "å¿«é€Ÿæ¨™è¨˜",
  "Random crop instead of center crop": "ä½¿ç”¨éš¨æ©Ÿè£åˆ‡ (è€Œéä¸­å¿ƒè£åˆ‡)",
  "Rank dropout": "ç¶­åº¦æ¨æ£„",
  "Rate of caption dropout": "æç¤ºè©æ¨æ£„æ¯”ä¾‹",
  "Recommended value of 0.5 when used": "è‹¥ä½¿ç”¨æ™‚ï¼Œå»ºè­°ä½¿ç”¨ 0.5",
  "Recommended value of 5 when used": "è‹¥ä½¿ç”¨æ™‚ï¼Œå»ºè­°ä½¿ç”¨ 5",
  "recommended values are 0.05 - 0.15": "è‹¥ä½¿ç”¨æ™‚ï¼Œå»ºè­°ä½¿ç”¨ 0.05 - 0.15",
  "Regularisation folder": "æ­£è¦åŒ–è³‡æ–™å¤¾",
  "Regularisation images": "æ­£è¦åŒ–åœ–ç‰‡",
  "Repeats": "é‡è¤‡",
  "Replacement text": "å–ä»£æ–‡å­—",
  "Required bitsandbytes >= 0.36.0": "éœ€è¦ bitsandbytes >= 0.36.0",
  "Resize LoRA": "èª¿æ•´ LoRA å°ºå¯¸",
  "Resize model": "èª¿æ•´æ¨¡å‹å¤§å°",
  "Resolution (width,height)": "è§£æåº¦ (å¯¬åº¦, é«˜åº¦) ",
  "Resource Contributions": "è³‡æºè²¢ç»è€…",
  "Resume from saved training state": "å¾å„²å­˜çš„ç‹€æ…‹ç¹¼çºŒè¨“ç·´",
  "Resume TI training": "æ¢å¾© TI è¨“ç·´",
  "Runpod": "Runpod",
  "Sample every n epochs": "æ¯ N å€‹æ™‚æœŸ (Epoch) é€²è¡Œç¯„æœ¬å–æ¨£",
  "Sample every n steps": "æ¯ N å€‹æ­¥æ•¸é€²è¡Œç¯„æœ¬å–æ¨£",
  "Sample image generation during training": "åœ¨è¨“ç·´æœŸé–“ç”Ÿæˆå–æ¨£åœ–ç‰‡",
  "Sample prompts": "å–æ¨£ç¯„æœ¬æç¤ºè©æç¤º",
  "Sample sampler": "å–æ¨£ç¯„æœ¬æ¡æ¨£å™¨",
  "Samples": "ç¯„æœ¬",
  "Save dtype": "å„²å­˜æ•¸æ“šé¡å‹",
  "Save every N epochs": "æ¯ N å€‹é€±æœŸ (Epoch) å„²å­˜",
  "Save every N steps": "æ¯ N å€‹æ­¥é©Ÿå„²å­˜",
  "Save last N steps state": "å„²å­˜æœ€å¾Œ N å€‹æ­¥é©Ÿçš„è¨“ç·´ç‹€æ…‹",
  "Save last N steps": "å„²å­˜æœ€å¾Œ N å€‹æ­¥é©Ÿ",
  "Save precision": "å„²å­˜ç²¾åº¦",
  "Save to": "å„²å­˜åˆ°",
  "Save trained model as": "å„²å­˜è¨“ç·´æ¨¡å‹ç‚º",
  "Save training state": "å„²å­˜è¨“ç·´ç‹€æ…‹",
  "Save": "å„²å­˜",
  "Scale v prediction loss": "ç¸®æ”¾ v é æ¸¬æå¤± (v prediction loss)",
  "Scale weight norms": "ç¸®æ”¾æ¬Šé‡æ¨™æº–",
  "SD Model": "SD æ¨¡å‹",
  "SDXL model": "SDXL æ¨¡å‹",
  "Set the Max resolution to at least 1024x1024, as this is the standard resolution for SDXL. ": "æœ€å¤§è§£æåº¦æœ€å°‘è¨­å®šç‚º 1024x1024ï¼Œå› ç‚ºé€™æ˜¯ SDXL çš„æ¨™æº–è§£æåº¦ã€‚",
  "Set the Max resolution to at least 1024x1024, as this is the standard resolution for SDXL.": "æœ€å¤§è§£æåº¦æœ€å°‘è¨­å®šç‚º 1024x1024ï¼Œå› ç‚ºé€™æ˜¯ SDXL çš„æ¨™æº–è§£æåº¦ã€‚",
  "Setup": "è¨­å®š",
  "SGDNesterov": "SGDNesterov",
  "SGDNesterov8bit": "SGDNesterov8bit",
  "Shuffle caption": "æ‰“äº‚æç¤ºè©",
  "Source LoRA": "ä¾†æº LoRA",
  "Source model type": "ä¾†æºæ¨¡å‹é¡å‹",
  "Source model": "ä¾†æºæ¨¡å‹",
  "Sparsity": "ç¨€ç–æ€§",
  "Stable Diffusion base model": "ç©©å®šæ“´æ•£åŸºç¤æ¨¡å‹",
  "Stable Diffusion original model: ckpt or safetensors file": "ç©©å®šæ“´æ•£åŸå§‹æ¨¡å‹ï¼šckpt æˆ– safetensors æª”æ¡ˆ",
  "Start tensorboard": "å•Ÿå‹• TensorBoard",
  "Start training": "é–‹å§‹è¨“ç·´",
  "Starting GUI Service": "å•Ÿå‹• GUI æœå‹™",
  "Stop tensorboard": "åœæ­¢ TensorBoard",
  "Stop text encoder training": "åœæ­¢æ–‡å­—ç·¨ç¢¼å™¨è¨“ç·´",
  "Stop training": "åœæ­¢è¨“ç·´",
  "style template": "é¢¨æ ¼æ¨£ç‰ˆ",
  "sv_fro": "sv_fro",
  "Target model folder": "ç›®æ¨™æ¨¡å‹è³‡æ–™å¤¾",
  "Target model name": "ç›®æ¨™æ¨¡å‹åç¨±",
  "Target model precision": "ç›®æ¨™æ¨¡å‹ç²¾åº¦",
  "Target model type": "ç›®æ¨™æ¨¡å‹é¡å‹",
  "Template": "æ¨£ç‰ˆ",
  "Text Encoder learning rate": "æ–‡å­—ç·¨ç¢¼å™¨å­¸ç¿’ç‡",
  "The fine-tuning can be done with 24GB GPU memory with the batch size of 1.": "å¾®èª¿å¯ä»¥å†ä½¿ç”¨ 1 å€‹æ‰¹æ¬¡å¤§å°çš„æƒ…æ³ä¸‹ï¼Œåœ¨ 24GB GPU è¨˜æ†¶é«”çš„ç‹€æ…‹ä¸‹å®Œæˆã€‚",
  "The GUI allows you to set the training parameters and generate and run the required CLI commands to train the model.": "æ­¤ GUI å…è¨±ä½ è¨­å®šè¨“ç·´åƒæ•¸ï¼Œä¸¦ç”¢ç”ŸåŸ·è¡Œæ¨¡å‹è¨“ç·´æ‰€éœ€è¦çš„ CLI æŒ‡ä»¤ã€‚",
  "This guide is a resource compilation to facilitate the development of robust LoRA models.": "è©²æŒ‡å—æ˜¯ä¸€å€‹è³‡æºå½™æ•´ï¼Œä»¥ä¿ƒé€²å¼·å¤§LoRAæ¨¡å‹çš„é–‹ç™¼ã€‚",
  "This section provide Dreambooth tools to help setup your datasetâ€¦": "é€™äº›é¸æ“‡å¹«åŠ©è¨­ç½®è‡ªå·±çš„è³‡æ–™é›†",
  "This section provide LoRA tools to help setup your datasetâ€¦": "æœ¬ç¯€æä¾› LoRA å·¥å…·ä»¥å¹«åŠ©æ‚¨è¨­ç½®è³‡æ–™é›†...",
  "This section provide Various Finetuning guides and informationâ€¦": "æœ¬ç¯€æä¾›å„ç¨®å¾®èª¿æŒ‡å—å’Œè¨Šæ¯",
  "This utility allows quick captioning and tagging of images.": "æ­¤å·¥å…·å…è¨±å¿«é€Ÿåœ°ç‚ºåœ–åƒæ·»åŠ æ¨™é¡Œå’Œæ¨™ç±¤ã€‚",
  "This utility allows you to create simple caption files for each image in a folder.": "æ­¤å·¥å…·å…è¨±æ‚¨ç‚ºè³‡æ–™å¤¾ä¸­çš„æ¯å€‹åœ–ç‰‡å»ºç«‹ç°¡å–®çš„æ¨™ç±¤æ–‡ä»¶ã€‚",
  "This utility can be used to convert from one stable diffusion model format to another.": "è©²å·¥å…·å¯ç”¨æ–¼å°‡ä¸€å€‹ç©©å®šæ“´æ•£æ¨¡å‹æ ¼å¼è½‰æ›ç‚ºå¦ä¸€ç¨®æ ¼å¼",
  "This utility can extract a DyLoRA network from a finetuned model.": "è©²å·¥å…·å¯ä»¥å¾å¾®èª¿æ¨¡å‹ä¸­æå– DyLoRA ç¶²çµ¡ã€‚",
  "This utility can extract a LoRA network from a finetuned model.": "è©²å·¥å…·å¯ä»¥å¾å¾®èª¿æ¨¡å‹ä¸­æå– LoRA ç¶²çµ¡ã€‚",
  "This utility can extract a LyCORIS LoCon network from a finetuned model.": "å·¥å…·å¯ä»¥å¾å¾®èª¿æ¨¡å‹ä¸­æå– LyCORIS LoCon ç¶²çµ¡ã€‚",
  "This utility can merge a LyCORIS model into a SD checkpoint.": "è©²å·¥å…·å¯ä»¥å°‡ LyCORIS æ¨¡å‹åˆä¸¦åˆ° SD æ¨¡å‹ä¸­ã€‚",
  "This utility can merge two LoRA networks together into a new LoRA.": "è©²å·¥å…·å¯ä»¥å°‡å…©å€‹ LoRA ç¶²çµ¡åˆä¸¦ç‚ºä¸€å€‹æ–°çš„ LoRAã€‚",
  "This utility can merge up to 4 LoRA together or alternatively merge up to 4 LoRA into a SD checkpoint.": "è©²å·¥å…·å¯ä»¥åˆä¸¦å¤šé” 4 å€‹LoRAï¼Œæˆ–è€…é¸æ“‡æ€§åœ°å°‡å¤šé” 4 å€‹ LoRA åˆä¸¦åˆ° SD æ¨¡å‹ä¸­ã€‚",
  "This utility can resize a LoRA.": "è©²å·¥å…·å¯ä»¥èª¿æ•´ LoRA çš„å¤§å°ã€‚",
  "This utility can verify a LoRA network to make sure it is properly trained.": "è©²å·¥å…·å¯ä»¥é©—è­‰ LoRA ç¶²çµ¡ä»¥ç¢ºä¿å…¶å¾—åˆ°é©ç•¶çš„è¨“ç·´ã€‚",
  "This utility uses BLIP to caption files for each image in a folder.": "æ­¤å·¥å…·ä½¿ç”¨ BLIP ç‚ºè³‡æ–™å¤¾ä¸­çš„æ¯å¼µåœ–åƒæ·»åŠ æ¨™ç±¤ã€‚",
  "This utility will create the necessary folder structure for the training images and optional regularization images needed for the kohys_ss Dreambooth/LoRA method to function correctly.": "æ­¤å·¥å…·å°‡ç‚º kohys_ss Dreambooth/LoRA æ–¹æ³•æ­£å¸¸é‹è¡Œæ‰€éœ€çš„è¨“ç·´åœ–ç‰‡å’Œæ­£è¦åŒ–åœ–ç‰‡ï¼ˆå¯é¸ï¼‰å»ºç«‹å¿…è¦çš„è³‡æ–™å¤¾çµæ§‹ã€‚",
  "This utility will ensure that each concept folder in the dataset folder is used equally during the training process of the dreambooth machine learning model, regardless of the number of images in each folder. It will do this by renaming the concept folders to indicate the number of times they should be repeated during training.": "æ­¤å·¥å…·å°‡ç¢ºä¿åœ¨è¨“ç·´ dreambooth æ©Ÿå™¨å­¸ç¿’æ¨¡å‹çš„éç¨‹ä¸­ï¼Œè³‡æ–™é›†è³‡æ–™å¤¾ä¸­çš„æ¯å€‹æ¦‚å¿µè³‡æ–™å¤¾éƒ½å°‡è¢«å¹³ç­‰åœ°ä½¿ç”¨ï¼Œç„¡è«–æ¯å€‹è³‡æ–™å¤¾ä¸­æœ‰å¤šå°‘åœ–åƒã€‚å®ƒå°‡é€šéé‡å‘½åæ¦‚å¿µè³‡æ–™å¤¾ä¾†æŒ‡ç¤ºåœ¨è¨“ç·´æœŸé–“æ‡‰é‡è¦†ä½¿ç”¨å®ƒå€‘çš„æ¬¡æ•¸ã€‚",
  "This utility will group images in a folder based on their aspect ratio.": "æ­¤å·¥å…·å°‡æ ¹æ“šå®ƒå€‘çš„ç¸±æ©«æ¯”å°‡æ–‡ä»¶å¤¾ä¸­çš„åœ–åƒåˆ†çµ„ã€‚",
  "This utility will use GIT to caption files for each images in a folder.": "æ­¤å·¥å…·ä½¿ç”¨ GIT ç‚ºè³‡æ–™å¤¾ä¸­çš„æ¯å¼µåœ–åƒæ·»åŠ æ¨™ç±¤ã€‚",
  "This utility will use WD14 to caption files for each images in a folder.": "æ­¤å·¥å…·ä½¿ç”¨ WD14 ç‚ºè³‡æ–™å¤¾ä¸­çš„æ¯å¼µåœ–åƒæ·»åŠ æ¨™ç±¤ã€‚",
  "Tips for SDXL training": "SDXL è¨“ç·´æç¤º",
  "Token string": "æ¨™è¨˜ç¬¦è™Ÿ",
  "Train a custom model using kohya finetune python code": "ä½¿ç”¨ kohya finetune Python ç¨‹å¼è¨“ç·´è‡ªå®šç¾©æ¨¡å‹",
  "Train a custom model using kohya train network LoRA python codeâ€¦": "ä½¿ç”¨ kohya LoRA Python ç¨‹å¼è¨“ç·´è‡ªå®šç¾©æ¨¡å‹",
  "Train batch size": "è¨“ç·´æ‰¹æ¬¡å¤§å°",
  "Train Network": "è¨“ç·´ç¶²çµ¡",
  "Train text encoder": "è¨“ç·´æ–‡å­—ç·¨ç¢¼å™¨",
  "Train U-Net only.": "åƒ…è¨“ç·´ U-Net",
  "Training config folder": "è¨“ç·´è¨­å®šè³‡æ–™å¤¾",
  "Training Image folder": "è¨“ç·´åœ–ç‰‡è³‡æ–™å¤¾",
  "Training images": "è¨“ç·´åœ–ç‰‡",
  "Training steps per concept per epoch": "æ¯å€‹é€±æœŸæ¯å€‹æ¦‚å¿µçš„è¨“ç·´æ­¥é©Ÿ",
  "Training": "è¨“ç·´",
  "Troubleshooting": "æ•…éšœæ’é™¤",
  "Tutorials": "æ•™å­¸",
  "Unet learning rate": "UNet å­¸ç¿’ç‡",
  "UNet linear projection": "UNet ç·šæ€§æŠ•å½±",
  "Upgrading": "å‡çº§",
  "Use --cache_text_encoder_outputs option and caching latents.": "ä½¿ç”¨ --cache_text_encoder_outputs é¸é …ä¾†æš«å­˜æ½›ç©ºé–“ã€‚",
  "Use Adafactor optimizer. RMSprop 8bit or Adagrad 8bit may work. AdamW 8bit doesnâ€™t seem to work.": "ä½¿ç”¨ Adafactor å„ªåŒ–å™¨ã€‚ RMSprop 8bit æˆ– Adagrad 8bit å¯èƒ½æœ‰æ•ˆã€‚ AdamW 8bit å¥½åƒç„¡æ³•é‹ä½œã€‚",
  "Use beam search": "ä½¿ç”¨ beam æœå°‹",
  "Use gradient checkpointing.": "ä½¿ç”¨æ¢¯åº¦æª¢æŸ¥é»ã€‚",
  "Use latent files": "ä½¿ç”¨æ½›ç©ºé–“æª”æ¡ˆ",
  "Use sparse biais": "ä½¿ç”¨ä½¿ç”¨ç¨€ç–åå·®",
  "Users can obtain and/or generate an api key in the their user settings on the website: https://wandb.ai/login": "ä½¿ç”¨è€…å¯ä»¥åœ¨ä»¥ä¸‹ç¶²ç«™çš„ç”¨æˆ¶è¨­å®šä¸­å–å¾—ï¼Œæˆ–ç”¢ç”Ÿ API é‡‘é‘°ï¼šhttps://wandb.ai/login",
  "V Pred like loss": "V é æ¸¬æå¤±",
  "Values greater than 0 will make the model more img2img focussed. 0 = image only": "å¤§æ–¼ 0 çš„æ•¸å€¼æœƒä½¿æ¨¡å‹æ›´åŠ èšç„¦åœ¨ img2img ä¸Šã€‚0 è¡¨ç¤ºåƒ…é—œæ³¨æ–¼åœ–åƒç”Ÿæˆ",
  "Values lower than 1000 will make the model more img2img focussed. 1000 = noise only": "å°æ–¼ 1000 çš„æ•¸å€¼æœƒä½¿æ¨¡å‹æ›´åŠ èšç„¦åœ¨ img2img ä¸Šã€‚1000 è¡¨ç¤ºåƒ…ä½¿ç”¨å™ªè²ç”Ÿæˆåœ–ç‰‡",
  "Vectors": "å‘é‡",
  "Verbose": "è©³ç´°è¼¸å‡º",
  "WANDB API Key": "WANDB API é‡‘é‘°",
  "WANDB Logging": "WANDB ç´€éŒ„",
  "WARNING! The use of this utility on the wrong folder can lead to unexpected folder renaming!!!": "è­¦å‘Šï¼åœ¨éŒ¯èª¤çš„è³‡æ–™å¤¾ä½¿ç”¨æ­¤å·¥å…·ï¼Œå¯èƒ½æœƒæ„å¤–å°è‡´è³‡æ–™å¤¾è¢«é‡æ–°å‘½åï¼ï¼ï¼",
  "WD14 Captioning": "WD14 æè©",
  "Windows Upgrade": "Windows å‡çº§",
  "Train a custom model using kohya dreambooth python codeâ€¦": "ä½¿ç”¨ kohya dreambooth Python ç¨‹å¼è¨“ç·´è‡ªå®šç¾©æ¨¡å‹",
  "Training comment": "è¨“ç·´è¨»è§£",
  "Train a TI using kohya textual inversion python codeâ€¦": "ä½¿ç”¨ kohya textual inversion Python ç¨‹å¼è¨“ç·´ TI æ¨¡å‹",
  "Train a custom model using kohya finetune python codeâ€¦": "ä½¿ç”¨ kohya finetune Python ç¨‹å¼è¨“ç·´è‡ªå®šç¾©æ¨¡å‹"
}
